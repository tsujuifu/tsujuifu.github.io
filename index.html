
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>Ray@Meta</title>
        
        <link rel="stylesheet" type="text/css" href="./assets/academicons.min.css">
        <script src="./assets/fontawesome.min.js"></script>

        <style>
            @import url("https://fonts.googleapis.com/css2?family=Nunito&display=swap");

            a {
                color: #1772d0;
                text-decoration: none;
            }
            a:focus, a:hover {
                color: #f09228;
                text-decoration: none;
            }
            hr {
                border: 0;
                height: 1px;
                background: gray;
            }
            div {
                display: inline-block;
                vertical-align: middle;
            }
        </style>
    </head>

    <body style="width: 600px; margin: auto; padding-top: 20px; font-family: 'Nunito';">
        <center>
            <div><img src="./assets/profile.jpg" style="width: 175px; border-radius: 10px;" /></div>
            <div style="padding-left: 20px; width: 400px; text-align: left;">
                <center style="font-size: 24px;">Tsu-Jui (Ray) Fu</center>
                <p>I am a senior researcher at <a href="https://ai.meta.com" target="_blank">Meta GenAI</a>. My research lies in <b>vision+language</b> and <b>unified visual generation</b>. I completed my Ph.D. in computer science from <a href="https://www.cs.ucsb.edu" target="_blank">UCSB</a>, advised by <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>. My goal is to bridge the gap between multiple modalities via the AI system.</p>
                <center>
                    <a href="mailto:tsujuifu@gmail.com" target="_blank"><i class="fas fa-envelope-square fa-2x"></i></a>&nbsp;
                    <a href="https://github.com/tsujuifu" target="_blank"><i class="fab fa-github-square fa-2x"></i></a>&nbsp;
                    <a href="./assets/cv.pdf" target="_blank"><i class="ai ai-cv-square ai-2x"></i></a>&nbsp;
                    <a href="https://scholar.google.com/citations?user=7QRDcC0AAAAJ" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a>&nbsp;
                    <a href="https://www.linkedin.com/in/tsujuifu1996" target="_blank"><i class="fab fa-linkedin fa-2x"></i></a>&nbsp;
                    <a href="https://www.instagram.com/tsujuifu" target="_blank"><i class="fab fa-instagram-square fa-2x"></i></a>&nbsp;
                </center>
            </div>
        </center>
        <hr />
        <p>
            2025 - Now <br />
            Senior Research Scientist @ <a href="https://ai.meta.com" target="_blank">Meta GenAI</a>
        </p>
        <p>
            2024 - 2025 <br />
            Research Scientist @ <a href="https://machinelearning.apple.com" target="_blank">Apple AI/ML</a>
        </p>
        <p>
            2019 - 2024 <br />
            Research Assistant @ <a href="http://nlp.cs.ucsb.edu" target="_blank">UCSB NLP</a>
        </p>
        <p>
            2018 - 2019 <br />
            Research Assistant @ <a href="https://ckip.iis.sinica.edu.tw" target="_blank">Academia Sinica CKIP</a>
        </p>
        <hr />
        <p>
            <b>GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing</b> <br />
            <a href="https://scholar.google.com/citations?user=kP0Q7L4AAAAJ" target="_blank">Yusu Qian</a>, <a href="https://scholar.google.com/citations?user=zP9K32EAAAAJ" target="_blank">Jiasen Lu</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YIMfCY0AAAAJ" target="_blank">Xinze Wang</a>, <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a>, <a href="https://scholar.google.com/citations?user=0YPYs5UAAAAJ" target="_blank">Wenze Hu</a>, and <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a> <br />
            arXiv:2505.11493 <br />
            <a href="https://arxiv.org/abs/2505.11493" target="_blank">Paper</a>
        </p>
        <p>
            <b>DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation</b> <br />
            <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=HrzHNbAAAAAJ" target="_blank">Rui Qian</a>, <a href="https://scholar.google.com/citations?user=0YPYs5UAAAAJ" target="_blank">Wenze Hu</a>, <b>Tsu-Jui Fu</b>, <a href="https://www.linkedin.com/in/jialing-tong" target="_blank">Jialing Tong</a>, <a href="https://scholar.google.com/citations?user=YIMfCY0AAAAJ" target="_blank">Xinze Wang</a>, <a href="https://www.linkedin.com/in/lezhili" target="_blank">Lezhi Li</a>, <a href="https://scholar.google.com/citations?user=nI3cKV8AAAAJ" target="_blank">Bowen Zhang</a>, <a href="https://scholar.google.com/citations?user=3B2c31wAAAAJ" target="_blank">Alex Schwing</a>, <a href="https://scholar.google.com/citations?user=yFMX138AAAAJ" target="_blank">Wei Liu</a>, and <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a> <br />
            arXiv:2503.10618 <br />
            <a href="https://arxiv.org/abs/2503.10618" target="_blank">Paper</a>
        </p>
        <p>
            <b>CAR: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</b> <br />
            <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=O0yBqysAAAAJ" target="_blank">Pengsheng Guo</a>, <a href="https://scholar.google.com/citations?user=Kl4T9FYAAAAJ" target="_blank">Liangchen Song</a>, <a href="https://scholar.google.com/citations?user=zP9K32EAAAAJ" target="_blank">Jiasen Lu</a>, <a href="https://scholar.google.com/citations?user=HrzHNbAAAAAJ" target="_blank">Rui Qian</a>, <a href="https://scholar.google.com/citations?user=YIMfCY0AAAAJ" target="_blank">Xinze Wang</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=yFMX138AAAAJ" target="_blank">Wei Liu</a>, <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a>, and <a href="https://scholar.google.com/citations?user=3B2c31wAAAAJ" target="_blank">Alex Schwing</a> <br />
            Neurips'25 <br />
            <a href="https://arxiv.org/abs/2509.19300" target="_blank">Paper</a>
        </p>
        <p>
            <b>UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=kP0Q7L4AAAAJ" target="_blank">Yusu Qian</a>, <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=0YPYs5UAAAAJ" target="_blank">Wenze Hu</a>, <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a>, and <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a> <br />
            ICCV'25 <br />
            <a href="https://arxiv.org/abs/2503.12652" target="_blank">Paper</a>
        </p>
        <p>
            <b>STIV: Scalable Text and Image Conditioned Video Generation</b> <br />
            <a href="https://scholar.google.com/citations?user=4ahRAd4AAAAJ" target="_blank">Zongyu Lin*</a>, <a href="https://scholar.google.com/citations?user=yFMX138AAAAJ" target="_blank">Wei Liu*</a>, <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=zP9K32EAAAAJ" target="_blank">Jiasen Lu</a>, <a href="https://scholar.google.com/citations?user=0YPYs5UAAAAJ" target="_blank">Wenze Hu</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=k97jb6gAAAAJ" target="_blank">Jesse Allardice</a>, <a href="https://scholar.google.com/citations?user=s_Ws1uYAAAAJ" target="_blank">Zhengfeng Lai</a>, <a href="https://scholar.google.com/citations?user=Kl4T9FYAAAAJ" target="_blank">Liangchen Song</a>, <a href="https://scholar.google.com/citations?user=nI3cKV8AAAAJ" target="_blank">Bowen Zhang</a>, <a href="https://www.linkedin.com/in/cha-chen-6469935a" target="_blank">Cha Chen</a>, <a href="https://www.linkedin.com/in/yiranfei" target="_blank">Yiran Fei</a>, <a href="https://scholar.google.com/citations?user=PMeFEOIAAAAJ" target="_blank">Yifan Jiang</a>, <a href="https://www.linkedin.com/in/lezhili" target="_blank">Lezhi Li</a>, <a href="https://scholar.google.com/citations?user=TQgOjK0AAAAJ" target="_blank">Yizhou Sun</a>, <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ" target="_blank">Kai-Wei Chang</a>, and <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a> <br />
            ICCV'25 <br />
            <a href="https://arxiv.org/abs/2412.07730" target="_blank">Paper</a>
        </p>
        <p>
            <b>TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation</b> <br />
            <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <a href="https://scholar.google.com/citations?user=xD-wj_0AAAAJ" target="_blank">Jiachen Li</a>, <a href="https://scholar.google.com/citations?user=pAlwjdgAAAAJ" target="_blank">Michael Saxon</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=U8ShbhUAAAAJ" target="_blank">Wenhu Chen</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            ACL'25 (Findings) <br />
            <a href="https://arxiv.org/abs/2406.08656" target="_blank">Paper</a> / <a href="https://weixi-feng.github.io/tc-bench" target="_blank">Project</a> / <a href="https://github.com/weixi-feng/tc-bench" target="_blank">Code</a>
        </p>
        <p>
            <b>From Text to Pixel: Advancing Long-Context Understanding in MLLMs</b> <br />
            <a href="https://scholar.google.com/citations?user=pcmr6GMAAAAJ" target="_blank">Yujie Lu</a>, <a href="https://scholar.google.com/citations?user=SW_WaQ0AAAAJ" target="_blank">Xiujun Li</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=G5dQztgAAAAJ" target="_blank">Miguel Eckstein</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            arXiv:2405.14213 <br />
            <a href="https://arxiv.org/abs/2405.14213" target="_blank">Paper</a>
        </p>
        <p>
            <b>Controllable Visual Editing via Natural Language</b> <br />
            <b>Tsu-Jui Fu</b> <br />
            <a href="https://escholarship.org/uc/item/67v7w9wh" target="_blank">PhD Dissertation</a>
        </p>
        <p>
            <b>T2V-Turbo: Breaking the Quality Bottleneck of Video Consistency Model with Mixed Reward Feedback</b> <br />
            <a href="https://scholar.google.com/citations?user=xD-wj_0AAAAJ" target="_blank">Jiachen Li</a>, <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=3vvbplcAAAAJ" target="_blank">Xinyi Wang</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, <a href="https://scholar.google.com/citations?user=U8ShbhUAAAAJ" target="_blank">Wenhu Chen</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            NeurIPS'24 <br />
            <a href="https://arxiv.org/abs/2405.18750" target="_blank">Paper</a> / <a href="https://t2v-turbo.github.io" target="_blank">Project</a> / <a href="https://github.com/Ji4chenLi/t2v-turbo" target="_blank">Code</a>
        </p>
        <p>
            <b>Discffusion: Discriminative Diffusion Models as Few-shot Vision and Language Learners</b> <br />
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ" target="_blank">Xuehai He</a>, <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=1Cv6Sf4AAAAJ" target="_blank">Varun Jampani</a>, <a href="https://scholar.google.com/citations?user=CNKX9bgAAAAJ" target="_blank">Arjun Akula</a>, <a href="https://scholar.google.com/citations?user=BV2dbjEAAAAJ" target="_blank">Pradyumna Narayana</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, and <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a> <br />
            TMLR'24 <br />
            <a href="https://arxiv.org/abs/2305.10722" target="_blank">Paper</a> / <a href="https://sites.google.com/view/discffusion" target="_blank">Project</a> / <a href="https://github.com/eric-ai-lab/Discffusion" target="_blank">Code</a>
        </p>
        <p>
            <b>Ferret-v2: An Improved Baseline for Referring and Grounding with Large Language Models</b> <br />
            <a href="https://scholar.google.com/citations?user=1vz0kKUAAAAJ" target="_blank">Haotian Zhang*</a>, <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ" target="_blank">Haoxuan You*</a>, <a href="https://scholar.google.com/citations?user=1Inet5oAAAAJ" target="_blank">Philipp Dufter</a>, <a href="https://scholar.google.com/citations?user=nI3cKV8AAAAJ" target="_blank">Bowen Zhang</a>, <a href="https://scholar.google.com/citations?user=gAKJLoIAAAAJ" target="_blank">Chen Chen</a>, <a href="https://scholar.google.com/citations?user=uxlU7J8AAAAJ" target="_blank">Hong-You Chen</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, <a href="https://scholar.google.com/citations?user=OMVTRscAAAAJ" target="_blank">Shih-Fu Chang</a>, <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a>, and <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a> <br />
            COLM'24 <br />
            <a href="https://arxiv.org/abs/2404.07973" target="_blank">Paper</a>
        </p>
        <p>
            <b>Guiding Instruction-based Image Editing via Multimodal Large Language Models</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=0YPYs5UAAAAJ" target="_blank">Wenze Hu</a>, <a href="https://scholar.google.com/citations?user=l1hP40AAAAAJ" target="_blank">Xianzhi Du</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a>, and <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a> <br />
            ICLR'24 (Spotlight) <br />
            <a href="https://arxiv.org/abs/2309.17102" target="_blank">Paper</a> / <a href="https://mllm-ie.github.io" target="_blank">Project</a> / <a href="./slides/iclr24_mgie.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/WoUqIdFfUz0" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/pytorch_mgie" target="_blank">Code</a>
        </p>
        <p>
            <b>VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View</b> <br />
            <a href="https://scholar.google.com/citations?user=EqJEek8AAAAJ" target="_blank">Raphael Schumann</a>, <a href="https://scholar.google.com/citations?user=xNWgry0AAAAJ" target="_blank">Wanrong Zhu</a>, <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=nY9tQLYAAAAJ" target="_blank">Stefan Riezler</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            AAAI'24 <br />
            <a href="https://arxiv.org/abs/2307.06082" target="_blank">Paper</a> / <a href="https://map2seq.schumann.pub/vln/velma" target="_blank">Project</a> / <a href="https://github.com/raphael-sch/VELMA" target="_blank">Code</a>
        </p>
        <p>
            <b>Text-guided 3D Human Generation from 2D Collections</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=J9_LwQUAAAAJ" target="_blank">Wenhan Xiong</a>, <a href="https://scholar.google.com/citations?user=g5QpITUAAAAJ" target="_blank">Yixin Nie</a>, <a href="https://scholar.google.com/citations?user=jidrykQAAAAJ" target="_blank">Jingyu Liu</a>, <a href="https://scholar.google.com/citations?user=iPmTQZMAAAAJ" target="_blank">Barlas Oğuz</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EMNLP'23 (Findings) <br />
            <a href="https://arxiv.org/abs/2305.14312" target="_blank">Paper</a> / <a href="https://text-3dh.github.io" target="_blank">Project</a> / <a href="./slides/emnlp23_t3h.ppsx" target="_blank">Slide</a> / <a href="https://youtu.be/PQyeC6du1Oo" target="_blank">Video</a> / <a href="https://drive.google.com/drive/folders/11aacVBBKba1CC1Pp1IO1Dnca1t35OMVs" target="_blank">Dataset</a>
        </p>
        <p>
            <b>EDIS: Entity-Driven Image Search over Multimodal Web Content</b> <br />
            <a href="https://www.linkedin.com/in/emerald-siqi-liu-a9b168194" target="_blank">Siqi Liu*</a>, <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng*</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=U8ShbhUAAAAJ" target="_blank">Wenhu Chen</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EMNLP'23 (Long) <br />
            <a href="https://arxiv.org/abs/2305.13631" target="_blank">Paper</a> / <a href="https://github.com/emerisly/EDIS" target="_blank">Code</a>
        </p>
        <p>
            <b>Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation</b> <br />
            <a href="https://scholar.google.com/citations?user=xNWgry0AAAAJ" target="_blank">Wanrong Zhu</a>, <a href="https://scholar.google.com/citations?user=3vvbplcAAAAJ" target="_blank">Xinyi Wang</a>, <a href="https://scholar.google.com/citations?user=pcmr6GMAAAAJ" target="_blank">Yujie Lu</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <a href="https://scholar.google.com/citations?user=G5dQztgAAAAJ" target="_blank">Miguel Eckstein</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EMNLP'23 (Short) <br />
            <a href="https://arxiv.org/abs/2305.11317" target="_blank">Paper</a>
        </p>
        <p>
            <b>Photoswap: Personalized Subject Swapping in Images</b> <br />
            <a href="https://scholar.google.com/citations?user=B3YeB3YAAAAJ" target="_blank">Jing Gu</a>, <a href="https://scholar.google.com/citations?user=fYqdLx4AAAAJ" target="_blank">Yilin Wang</a>, <a href="https://www.semanticscholar.org/author/Nanxuan-Zhao/51150125" target="_blank">Nanxuan Zhao</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=uO0k6DMAAAAJ" target="_blank">Wei Xiong</a>, <a href="https://scholar.google.com/citations?user=1ytghtEAAAAJ" target="_blank">Qing Liu</a>, <a href="https://scholar.google.com/citations?user=HuerflQAAAAJ" target="_blank">Zhifei Zhang</a>, <a href="https://scholar.google.com/citations?user=HZLiJt0AAAAJ" target="_blank">He Zhang</a>, <a href="https://scholar.google.com/citations?user=TkVHKDgAAAAJ" target="_blank">Jianming Zhang</a>, <a href="https://scholar.google.com/citations?user=dX3FbO4AAAAJ" target="_blank">Hyunjoon Jung</a>, and <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a> <br />
            NeurIPS'23 <br />
            <a href="https://arxiv.org/abs/2305.18286" target="_blank">Paper</a> / <a href="https://photoswap.github.io" target="_blank">Project</a> / <a href="./slides/neurips23_photoswap.pdf" target="_blank">Slide</a> / <a href="https://github.com/eric-ai-lab/photoswap" target="_blank">Code</a>
        </p>
        <p>
            <b>LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</b> <br />
            <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng*</a>, <a href="https://scholar.google.com/citations?user=xNWgry0AAAAJ" target="_blank">Wanrong Zhu*</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=1Cv6Sf4AAAAJ" target="_blank">Varun Jampani</a>, <a href="https://scholar.google.com/citations?user=CNKX9bgAAAAJ" target="_blank">Arjun Akula</a>, <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ" target="_blank">Xuehai He</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            NeurIPS'23 <br />
            <a href="https://arxiv.org/abs/2305.15393" target="_blank">Paper</a> / <a href="https://layoutgpt.github.io" target="_blank">Project</a> / <a href="https://github.com/weixi-feng/LayoutGPT" target="_blank">Code</a>
        </p>
        <p>
            <b>Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=pwpweRQAAAAJ" target="_blank">Licheng Yu</a>, <a href="https://scholar.google.com/citations?user=DplAah0AAAAJ" target="_blank">Ning Zhang</a>, <a href="https://scholar.google.com/citations?user=IYDJuOAAAAAJ" target="_blank">Cheng-Yang Fu</a>, <a href="https://scholar.google.com/citations?user=jeKOwvsAAAAJ" target="_blank">Jong-Chyi Su</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, and <a href="https://scholar.google.com/citations?user=xY1GdVgAAAAJ" target="_blank">Sean Bell</a> <br />
            CVPR'23 <br />
            <a href="https://arxiv.org/abs/2211.12824" target="_blank">Paper</a> / <a href="https://tvc-mmvg.github.io" target="_blank">Project</a> / <a href="./slides/cvpr23_tvc.ppsx" target="_blank">Slide</a> / <a href="https://youtu.be/dnBzUfsf9Cc" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/pytorch_tvc" target="_blank">Code</a>
        </p>
        <p>
            <b>An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling</b> <br />
            <b>Tsu-Jui Fu*</b>, <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ" target="_blank">Linjie Li*</a>, <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a>, <a href="https://scholar.google.com/citations?user=LKSy1kwAAAAJ" target="_blank">Kevin Lin</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ" target="_blank">Lijuan Wang</a>, and <a href="https://scholar.google.com/citations?user=bkALdvsAAAAJ" target="_blank">Zicheng Liu</a> <br />
            CVPR'23 <br />
            <a href="https://arxiv.org/abs/2209.01540" target="_blank">Paper</a> / <a href="./slides/cvpr23_empirical-mvm.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/T1qTkcMCq1k" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/pytorch_empirical-mvm" target="_blank">Code</a>
        </p>
        <p>
            <b>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</b> <br />
            <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ" target="_blank">Xuehai He</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=1Cv6Sf4AAAAJ" target="_blank">Varun Jampani</a>, <a href="https://scholar.google.com/citations?user=CNKX9bgAAAAJ" target="_blank">Arjun Akula</a>, <a href="https://scholar.google.com/citations?user=BV2dbjEAAAAJ" target="_blank">Pradyumna Narayana</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            ICLR'23 <br />
            <a href="https://arxiv.org/abs/2212.05032" target="_blank">Paper</a> / <a href="https://weixi-feng.github.io/structure-diffusion-guidance" target="_blank">Project</a> / <a href="https://github.com/weixi-feng/Structured-Diffusion-Guidance" target="_blank">Code</a>
        </p>
        <p>
            <b>ULN: Towards Underspecified Vision-and-Language Navigation</b> <br />
            <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=pcmr6GMAAAAJ" target="_blank">Yujie Lu</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EMNLP'22 (Long) <br />
            <a href="https://arxiv.org/abs/2210.10020" target="_blank">Paper</a> / <a href="./slides/emnlp22_uln.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/axHJ7Ty1h60" target="_blank">Video</a> / <a href="https://github.com/weixi-feng/ULN" target="_blank">Code</a>
        </p>
        <p>
            <b>CPL: Counterfactual Prompt Learning for Vision and Language Models</b> <br />
            <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ" target="_blank">Xuehai He</a>, <a href="https://dblp.org/pid/234/1212" target="_blank">Diji Yang</a>, <a href="https://scholar.google.com/citations?user=ihuH8uMAAAAJ" target="_blank">Weixi Feng</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=CNKX9bgAAAAJ" target="_blank">Arjun Akula</a>, <a href="https://scholar.google.com/citations?user=1Cv6Sf4AAAAJ" target="_blank">Varun Jampani</a>, <a href="https://scholar.google.com/citations?user=BV2dbjEAAAAJ" target="_blank">Pradyumna Narayana</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, and <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a> <br />
            EMNLP'22 (Long) <br />
            <a href="https://arxiv.org/abs/2210.10362" target="_blank">Paper</a> / <a href="https://youtu.be/t-M7N4UeZSQ" target="_blank">Video</a> / <a href="https://github.com/eric-ai-lab/CPL" target="_blank">Code</a>
        </p>
        <p>
            <b>Language-Driven Artistic Style Transfer</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            ECCV'22 <br />
            <a href="https://arxiv.org/abs/2106.00178" target="_blank">Paper</a> / <a href="https://ldast.github.io" target="_blank">Project</a> / <a href="./slides/eccv22_ldast.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/76wKrkJjsgg" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/pytorch_ldast" target="_blank">Code</a>
        </p>
        <p>
            <b>M<sup>3</sup>L: Language-based Video Editing via Multi-Modal Multi-Level Transformer</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <a href="https://scholar.google.com/citations?user=7yJze9oAAAAJ" target="_blank">Scott Grafton</a>, <a href="https://scholar.google.com/citations?user=G5dQztgAAAAJ" target="_blank">Miguel Eckstein</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            CVPR'22 <br />
            <a href="https://arxiv.org/abs/2104.01122" target="_blank">Paper</a> / <a href="./slides/cvpr22_m3l.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/cbjYYFy9rYk" target="_blank">Video</a> / <a href="https://drive.google.com/drive/folders/1y41XPkktpy_wmqJm4w2kecpprAWNlOrq" target="_blank">Dataset</a>
        </p>
        <p>
            <b>DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, <a href="https://scholar.google.com/citations?user=m7Jr-b4AAAAJ" target="_blank">Daniel McDuff</a>, and <a href="https://scholar.google.com/citations?user=dNHNpxoAAAAJ" target="_blank">Yale Song</a> <br />
            AAAI'22 <br />
            <a href="https://arxiv.org/abs/2101.11796" target="_blank">Paper</a> / <a href="https://doc2ppt.github.io" target="_blank">Project</a> / <a href="./slides/aaai22_doc2ppt.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/-phY1ZhOzcY" target="_blank">Video</a> / <a href="https://drive.google.com/drive/folders/1s2zJ04WZYifZhotRCXpk4OGtCHWXuM0b" target="_blank">Code</a>
        </p>
        <p>
            <b>VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ" target="_blank">Linjie Li</a>, <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a>, <a href="https://scholar.google.com/citations?user=LKSy1kwAAAAJ" target="_blank">Kevin Lin</a>, <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a>, <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ" target="_blank">Lijuan Wang</a>, and <a href="https://scholar.google.com/citations?user=bkALdvsAAAAJ" target="_blank">Zicheng Liu</a> <br />
            arXiv:2111.12681 <br />
            <a href="https://arxiv.org/abs/2111.12681" target="_blank">Paper</a> / <a href="https://github.com/tsujuifu/pytorch_violet" target="_blank">Code</a>
        </p>
        <p>
            <b>H-FND: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction</b> <br />
            <a href="https://dblp.org/pid/193/3549" target="_blank">Jhih-Wei Chen*</a>, <b>Tsu-Jui Fu*</b>, <a href="https://www.linkedin.com/in/chen-kang-lee-64058a18b" target="_blank">Chen-Kang Lee</a>, and <a href="https://scholar.google.com/citations?user=AHG3DncAAAAJ" target="_blank">Wei-Yun Ma</a> <br />
            ACL'21 (Findings) <br />
            <a href="https://arxiv.org/abs/2012.03536" target="_blank">Paper</a> / <a href="./slides/acl21_h-fnd.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/Q2O5fQQdH_0" target="_blank">Video</a> / <a href="https://github.com/ckiplab/hfnd" target="_blank">Code</a>
        </p>
        <p>
            <b>Semi-Supervised Policy Initialization for Playing Games with Language Hints</b> <br />
            <b>Tsu-Jui Fu</b> and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            NAACL'21 (Short) <br />
            <a href="https://aclanthology.org/2021.naacl-main.249" target="_blank">Paper</a> / <a href="./slides/naacl21_ssi.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/5UbeIKX4voY" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/code_ssi" target="_blank">Code</a>
        </p>
        <p>
            <b>L2C: Describing Visual Differences Needs Semantic Understanding of Individuals</b> <br />
            <a href="https://scholar.google.com/citations?user=7I_zqNoAAAAJ" target="_blank">An Yan</a>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <b>Tsu-Jui Fu</b>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EACL'21 (Short) <br />
            <a href="https://arxiv.org/abs/2102.01860" target="_blank">Paper</a> / <a href="./slides/eacl21_l2c.pdf" target="_blank">Slide</a>
        </p>
        <p>
            <b>Multimodal Style Transfer Learning for Outdoor Vision-and-Language Navigation</b> <br />
            <a href="https://scholar.google.com/citations?user=xNWgry0AAAAJ" target="_blank">Wanrong Zhu</a>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=7I_zqNoAAAAJ" target="_blank">An Yan</a>, <a href="https://scholar.google.com/citations?user=BV2dbjEAAAAJ" target="_blank">Pradyumna Narayana</a>, <a href="https://dblp.org/pid/234/3599" target="_blank">Kazoo Sone</a>, <a href="https://scholar.google.com/citations?user=0kV69XQAAAAJ" target="_blank">Sugato Basu</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EACL'21 (Long) <br />
            <a href="https://arxiv.org/abs/2007.00229" target="_blank">Paper</a> / <a href="./slides/eacl21_mtst.pdf" target="_blank">Slide</a> / <a href="https://github.com/VegB/VLN-Transformer" target="_blank">Code</a>
        </p>
        <p>
            <b>SSCR: Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <a href="https://scholar.google.com/citations?user=7yJze9oAAAAJ" target="_blank">Scott Grafton</a>, <a href="https://scholar.google.com/citations?user=G5dQztgAAAAJ" target="_blank">Miguel Eckstein</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            EMNLP'20 (Oral) <br />
            <a href="https://arxiv.org/abs/2009.09566" target="_blank">Paper</a> / <a href="./slides/emnlp20_sscr.pdf" target="_blank">Slide</a> / <a href="https://github.com/tsujuifu/pytorch_sscr" target="_blank">Code</a>
        </p>
        <p>
            <b>Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=YjqluE0AAAAJ" target="_blank">Xin Eric Wang</a>, <a href="https://scholar.google.com/citations?user=E0aog_kAAAAJ" target="_blank">Matthew Peterson</a>, <a href="https://scholar.google.com/citations?user=7yJze9oAAAAJ" target="_blank">Scott Grafton</a>, <a href="https://scholar.google.com/citations?user=G5dQztgAAAAJ" target="_blank">Miguel Eckstein</a>, and <a href="https://scholar.google.com/citations?user=gf8Ms_8AAAAJ" target="_blank">William Yang Wang</a> <br />
            ECCV'20 (Spotlight) <br />
            <a href="https://arxiv.org/abs/1911.07308" target="_blank">Paper</a> / <a href="./slides/eccv20_aps.pdf" target="_blank">Slide</a> / <a href="https://youtu.be/eCPtNWDe2RQ" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/model_aps" target="_blank">Model</a>
        </p>
        <p>
            <b>Why Attention? Analyzing and Remedying BiLSTM Deficiency in Modeling Cross-Context for NER</b> <br />
            <a href="https://scholar.google.com/citations?user=sqYoxbsAAAAJ" target="_blank">Peng-Hsuan Li</a>, <b>Tsu-Jui Fu</b>, and <a href="https://scholar.google.com/citations?user=AHG3DncAAAAJ" target="_blank">Wei-Yun Ma</a> <br />
            AAAI'20 (Oral) <br />
            <a href="https://arxiv.org/abs/1910.02586" target="_blank">Paper</a> / <a href="https://github.com/jacobvsdanniel/cross-ner" target="_blank">Code</a>
        </p>
        <p>
            <b>Learning from Observation-Only Demonstration for Task-Oriented Language Grounding via Self-Examination</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=3JyuzrwAAAAJ" target="_blank">Yuta Tsuboi</a>, <a href="https://scholar.google.com/citations?user=VY6PqvsAAAAJ" target="_blank">Sosuke Kobayashi</a>, and <a href="https://scholar.google.com/citations?user=UwuggM4AAAAJ" target="_blank">Yuta Kikuchi</a> <br />
            NeurIPSW'19 (ViGIL workshop) <br />
            <a href="https://dblp.dagstuhl.de/rec/conf/nips/FuTKK19" target="_blank">Paper</a>
        </p>
        <p>
            <b>A Distributed Scheme for Accelerating Semantic Video Segmentation on An Embedded Cluster</b> <br />
            <a href="https://scholar.google.com/citations?user=54gn_yUAAAAJ" target="_blank">Hsuan-Kung Yang*</a>, <b>Tsu-Jui Fu*</b>, <a href="https://www.semanticscholar.org/author/Kuan-Wei-Ho/51268109" target="_blank">Kuan-Wei Ho</a>, <a href="https://scholar.google.com/citations?user=y8ueJlIAAAAJ" target="_blank">Po-Han Chiang</a>, and <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ" target="_blank">Chun-Yi Lee</a> <br />
            ICCD'19 (Oral) <br />
            <a href="https://ieeexplore.ieee.org/document/8988729" target="_blank">Paper</a> / <a href="https://www.facebook.com/chunyilee/posts/10157957321859273" target="_blank">Video</a>
        </p>
        <p>
            <b>Adversarial Active Exploration for Inverse Dynamics Model Learning</b> <br />
            <a href="https://scholar.google.com/citations?user=GZkyN4cAAAAJ" target="_blank">Zhang-Wei Hong</a>, <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=4oXvi88AAAAJ" target="_blank">Tzu-Yun Shann</a>, <a href="https://scholar.google.com/citations?user=MGE-nvwAAAAJ" target="_blank">Yi-Hsiang Chang</a>, and <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ" target="_blank">Chun-Yi Lee</a> <br />
            CoRL'19 (Oral) <br />
            <a href="https://arxiv.org/abs/1806.10019" target="_blank">Paper</a>
        </p>
        <p>
            <b>GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://scholar.google.com/citations?user=sqYoxbsAAAAJ" target="_blank">Peng-Hsuan Li</a>, and <a href="https://scholar.google.com/citations?user=AHG3DncAAAAJ" target="_blank">Wei-Yun Ma</a> <br />
            ACL'19 (Long) <br />
            <a href="https://aclanthology.org/P19-1136" target="_blank">Paper</a> / <a href="./slides/acl19_graph-rel.pdf" target="_blank">Slide</a> / <a href="https://github.com/tsujuifu/pytorch_graph-rel" target="_blank">Code</a>
        </p>
        <p>
            <b>Attentive and Adversarial Learning for Video Summarization</b> <br />
            <b>Tsu-Jui Fu</b>, <a href="https://dblp.org/pid/214/6528" target="_blank">Shao-Heng Tai</a>, and <a href="https://scholar.google.com/citations?user=mwFy9kkAAAAJ" target="_blank">Hwann-Tzong Chen</a> <br />
            WACV'19 (Oral) <br />
            <a href="https://ieeexplore.ieee.org/document/8658673" target="_blank">Paper</a> / <a href="https://youtu.be/0irqOrpAYgw" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/pytorch_vsum-ptr-gan" target="_blank">Code</a>
        </p>
        <p>
            <b>Region-Semantics Preserving Image Synthesis</b> <br />
            <a href="https://dblp.org/pid/191/6724" target="_blank">Kang-Jun Liu</a>, <b>Tsu-Jui Fu</b>, and <a href="https://scholar.google.com/citations?user=Pw-5jKcAAAAJ" target="_blank">Shan-Hung Wu</a> <br />
            ACCV'18 <br />
            <a href="https://dl.acm.org/doi/10.1007/978-3-030-20870-7_20" target="_blank">Paper</a> / <a href="https://youtu.be/UwBjSUpjZU8" target="_blank">Video</a> / <a href="https://github.com/tsujuifu/tensorflow_preserving-gan" target="_blank">Code</a>
        </p>
        <p>
            <b>Diversity-Driven Exploration Strategy for Deep Reinforcement Learning</b> <br />
            <a href="https://scholar.google.com/citations?user=GZkyN4cAAAAJ" target="_blank">Zhang-Wei Hong</a>, <a href="https://scholar.google.com/citations?user=4oXvi88AAAAJ" target="_blank">Tzu-Yun Shann</a>, <a href="https://scholar.google.com/citations?user=peqCV80AAAAJ" target="_blank">Shih-Yang Su</a>, <a href="https://scholar.google.com/citations?user=MGE-nvwAAAAJ" target="_blank">Yi-Hsiang Chang</a>, <b>Tsu-Jui Fu</b>, and <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ" target="_blank">Chun-Yi Lee</a> <br />
            NeurIPS'18 <br />   
            <a href="https://arxiv.org/abs/1802.04564" target="_blank">Paper</a> / <a href="https://youtu.be/CxKT5ua-w4U" target="_blank">Video</a>
        </p>
        <p>
            <b>Speed Reading: Learning to Read ForBackward via Shuttle</b> <br />
            <b>Tsu-Jui Fu</b> and <a href="https://scholar.google.com/citations?user=AHG3DncAAAAJ" target="_blank">Wei-Yun Ma</a> <br />
            EMNLP'18 (Long) <br />
            <a href="https://aclanthology.org/D18-1474" target="_blank">Paper</a> / <a href="https://github.com/tsujuifu/pytorch_lstm-shuttle" target="_blank">Code</a>
        </p>
        <p>
            <b>Visual Relationship Prediction via Label Clustering and Incorporation of Depth Information</b> <br />
            <a href="https://scholar.google.com/citations?user=54gn_yUAAAAJ" target="_blank">Hsuan-Kung Yang</a>, <a href="https://scholar.google.com/citations?user=Zoiu7FsAAAAJ" target="_blank">An-Chieh Cheng*</a>, <a href="https://www.semanticscholar.org/author/Kuan-Wei-Ho/51268109" target="_blank">Kuan-Wei Ho*</a>, <b>Tsu-Jui Fu</b>, and <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ" target="_blank">Chun-Yi Lee</a> <br />
            ECCVW'18 (PIC workshop) <br />
            <a href="https://arxiv.org/abs/1809.02945" target="_blank">Paper</a>
        </p>
        <p>
            <b>Dynamic Video Segmentation Network</b> <br />
            <a href="https://scholar.google.com/citations?user=QA3VSaYAAAAJ" target="_blank">Yu-Syuan Xu</a>, <b>Tsu-Jui Fu*</b>, <a href="https://scholar.google.com/citations?user=54gn_yUAAAAJ" target="_blank">Hsuan-Kung Yang*</a>, and <a href="https://scholar.google.com/citations?user=5mYNdo0AAAAJ" target="_blank">Chun-Yi Lee</a> <br />
            CVPR'18 <br />
            <a href="https://arxiv.org/abs/1804.00931" target="_blank">Paper</a> / <a href="https://youtu.be/vadYHOyUVXs" target="_blank">Video</a> / <a href="https://github.com/XUSean0118/DVSNet" target="_blank">Code</a>
        </p>
        <hr />
        <p>
            Summer 2023 <br />
            Research Intern @ <a href="https://machinelearning.apple.com" target="_blank">Apple AI/ML</a> <br />
            Advisor: <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a> and <a href="https://scholar.google.com/citations?user=kvDbu90AAAAJ" target="_blank">Yinfei Yang</a>
        </p>
        <p>
            Summer 2022 <br />
            Research Intern @ <a href="https://ai.facebook.com" target="_blank">Meta AI</a> <br />
            Advisor: <a href="https://scholar.google.com/citations?user=pwpweRQAAAAJ" target="_blank">Licheng Yu</a> and <a href="https://scholar.google.com/citations?user=xY1GdVgAAAAJ" target="_blank">Sean Bell</a>
        </p>
        <p>
            Summer 2021 <br />
            Research Intern @ <a href="https://www.microsoft.com/en-us/research/group/azure-florence" target="_blank">Microsoft Azure AI</a> <br />
            Advisor: <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ" target="_blank">Linjie Li</a>, <a href="https://scholar.google.com/citations?user=E64XWyMAAAAJ" target="_blank">Zhe Gan</a>, and <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ" target="_blank">Lijuan Wang</a>
        </p>
        <p>
            Summer 2020 <br />
            Research Intern @ <a href="https://www.microsoft.com/en-us/research/group/vision" target="_blank">Microsoft Research</a> <br />
            Advisor: <a href="https://scholar.google.com/citations?user=dNHNpxoAAAAJ" target="_blank">Yale Song</a> and <a href="https://scholar.google.com/citations?user=m7Jr-b4AAAAJ" target="_blank">Daniel McDuff</a>
        </p>
        <p>
            Summer 2019 <br />
            Research Intern @ <a href="https://www.preferred-networks.jp/en" target="_blank">Preferred Networks</a> <br />
            Advisor: <a href="https://scholar.google.com/citations?user=3JyuzrwAAAAJ" target="_blank">Yuta Tsuboi</a> and <a href="https://scholar.google.co.uk/citations?user=w4d5WRcAAAAJ" target="_blank">Jason Naradowsky</a>
        </p>
    </body>
</html>
